{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zBdLoXOM7_U"
   },
   "source": [
    "**Inviscid Burgers’ Equation**\n",
    "\n",
    "For a given field $u(x, t)$ and diffusion coefficient (or kinematic viscosity, as in the original fluid mechanical context)  $\\nu$ , the general form of Burgers' equation (also known as viscous Burgers' equation) in one space dimension is the dissipative system:\n",
    "\n",
    "Burger's equation is the following:\n",
    "\n",
    "$$\\frac{\\partial u(x, t)}{\\partial t}+u \\frac{\\partial u(x, t)}{\\partial x}=\\nu \\frac{\\partial^{2} u(x, t)}{\\partial x^{2}}$$\n",
    "A special solution to the above PDE is:\n",
    "\n",
    "$$u(x, t)=1-\\tanh \\frac{x-x_{c}-t}{2 \\nu}$$\n",
    "\n",
    "here i define $x_c$ = 0:\n",
    "$$u(x, t)=1-\\tanh \\frac{x-t}{2 \\nu}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{c}\n",
    "\\frac{\\partial u}{\\partial t}+u \\frac{\\partial u}{\\partial x}=\\nu \\frac{\\partial^{2} u(x, t)}{\\partial x^{2}}, (x,t)∈[0,1] × [0,1]  \\\\\n",
    "u(x, 0)=F(x)=1-\\tanh \\frac{x}{2 \\nu}, x∈[0,1]\\\\\n",
    "u(x,t) = g(x,t) = 1-\\tanh \\frac{x-t}{2 \\nu}, x ∈ \\{0, 1\\}\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEGuH8yTPMEY"
   },
   "source": [
    "## 1.Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DNHUWKclMpM9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# %%\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "# check pytorch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8fRkZVbTZ4z"
   },
   "source": [
    "\n",
    "## 2.Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cSJ5MpJJMpU3"
   },
   "outputs": [],
   "source": [
    "dimension = 2\n",
    "input_width,layer_width = dimension, 300\n",
    "layer_depth = 5\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzmN2j4uTeYV"
   },
   "source": [
    "## 3.Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h-p2XB8STdwt"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# activation function\n",
    "def activation(x):\n",
    "    return torch.mul(x, torch.sigmoid(x))\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_width,layer_width,layer_depth):\n",
    "        super(Net,self).__init__()\n",
    "        self.layer_in = torch.nn.Linear(input_width, layer_width)\n",
    "        for i in range(layer_depth-1):\n",
    "            setattr(self, 'layer'+str(i), torch.nn.Linear(layer_width, layer_width))\n",
    "        self.layer_out = torch.nn.Linear(layer_width, 1)\n",
    "    def forward(self,x,layer_depth):\n",
    "        y = activation(self.layer_in(x))\n",
    "        for i in range(layer_depth-1):\n",
    "            y = activation(getattr(self, 'layer'+str(i))(y))\n",
    "        output = self.layer_out(y)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMjUd14qUDNN"
   },
   "source": [
    "${\\rm N}(\\vec{x}) := {\\rm A}_{\\rm 4}(\\sigma{\\rm A}_3({\\sigma}{\\rm A}_2({\\sigma}{\\rm A}_{\\rm 1}(\\vec{x}))))\\ ,$, where\n",
    "\n",
    "\n",
    "${\\rm A}_{\\rm 1}: \\mathbb{R}^2{→}\\mathbb{R}^w$, ${\\rm A}_2: \\mathbb{R}^w{→}\\mathbb{R}^w$, ${\\rm A}_3: \\mathbb{R}^w{→}\\mathbb{R}^w$, ${\\rm A}_{\\rm 4}: \\mathbb{R}^w{→}\\mathbb{R}$, \n",
    "\n",
    "note: there are $2w^2+6w+1$ parameters need to be trained in total in ${\\rm N}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khgnFBJrUOcX"
   },
   "source": [
    "## 4.Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SInmNlTtUPJf"
   },
   "source": [
    "### 4.1 Initialize the weights using Xavier normal initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5rf372itUCzN"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Xavier normal initialization for weights:\n",
    "#             mean = 0 std = gain * sqrt(2 / fan_in + fan_out)\n",
    "# zero initialization for biases\n",
    "def initialize_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZtqofB3yq_P"
   },
   "source": [
    "### 4.2 Define $u_0(x)$\n",
    "\n",
    "$$ [0,1] {\\ni} x {↦} u_0(x) =1- \\tanh \\frac{x}{2 \\nu}{∈} {\\mathbb R} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oSdzqfN6z2ei"
   },
   "outputs": [],
   "source": [
    "# Define u_0\n",
    "def u_0x(X):\n",
    "    x = X[:,0]\n",
    "    temp1 = torch.sub(1,torch.tanh(x/(2*nu)))\n",
    "    u_temp = temp1\n",
    "    return u_temp.reshape([X.size()[0], 1]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCimiC5QzLHU"
   },
   "source": [
    "### 4.3 Define the exact solution\n",
    "\n",
    "$$[0,1]{×}[0,1]{\\ni}(x, t) {↦} u(x, t)=1-\\tanh \\frac{x-t}{2 \\nu} {∈} {\\mathbb R}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6BS_ARXTMpX0"
   },
   "outputs": [],
   "source": [
    "# defination of exact solution\n",
    "def u_ex(X):     \n",
    "    x = X[:,0]\n",
    "    t = X[:,1]\n",
    "    \n",
    "    temp1 = torch.sub(1,torch.tanh((x-t)/(2*nu)))\n",
    "    u_temp = temp1\n",
    "    return u_temp.reshape([x.size()[0], 1]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GZwLSg93WpJ"
   },
   "source": [
    "### 4.4 Define g(x,t)\n",
    "\n",
    "$$ \\{0,1\\}×[0,1] \\ni (x,t) ↦ g(x,t) = 1-\\tanh \\frac{x-t}{2 \\nu} ∈ {\\mathbb R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2UggOvPi3YHg"
   },
   "outputs": [],
   "source": [
    "def g_x(X):     \n",
    "    x = X[:,0].to(device)\n",
    "    t = X[:,1].to(device)\n",
    "    temp1 = torch.sub(1,torch.tanh((x-t)/(2*nu)))\n",
    "    u_temp = temp1\n",
    "    return u_temp.reshape([x.size()[0], 1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2xibQeQY3YPR"
   },
   "outputs": [],
   "source": [
    "def g_0x(X):\n",
    "    x = torch.zeros([X.shape[0],1])[:,0].to(device)\n",
    "    x.requires_grad = True\n",
    "    t = X[:,1].to(device)\n",
    "    temp1 = torch.sub(1,torch.tanh((x-t)/(2*nu)))\n",
    "    u_temp = temp1\n",
    "    return u_temp.reshape([x.size()[0], 1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5ZolL9KF3YnB"
   },
   "outputs": [],
   "source": [
    "def g_1x(X):\n",
    "    x = torch.ones([X.shape[0],1])[:,0].to(device)\n",
    "    x.requires_grad = True\n",
    "    t = X[:,1].to(device)\n",
    "    temp1 = torch.sub(1,torch.tanh((x-t)/(2*nu)))\n",
    "    u_temp = temp1\n",
    "    return u_temp.reshape([x.size()[0], 1]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzn3REDL-uwv"
   },
   "source": [
    "### 4.4 Build the model\n",
    "\n",
    "$x = {\\vec X}[:,0]$\n",
    "\n",
    "$t = {\\vec X}[:,1]$\n",
    "\n",
    "Model for training on initial and boundary:$${\\rm model}({x,t}) := {\\rm N}(x, t)  $$\n",
    "Model for training on boundary:$${\\rm model}({x,t}) := {\\rm N}(x, t) ⋅ x ⋅ (1-x)  +  (1-x) ⋅ g(0,t) + x{⋅} g(1,t)$$\n",
    "Model for training on initial:$${\\rm model}({x,t}) := {\\rm N}(x, t)  ⋅ t  +  u_0(x) \\cdot (1-t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zI5edElLMpaa"
   },
   "outputs": [],
   "source": [
    "def model(X,initial = False, boundary = False):\n",
    "    x = X[:,0]\n",
    "    t = X[:,1].reshape([x.size()[0], 1])\n",
    "    x_temp = x.reshape([x.size()[0], 1])\n",
    "    t_temp = t.reshape([t.size()[0], 1])\n",
    "    x_minus = (1.0-x).reshape([x.size()[0], 1])\n",
    "    t_minus = (1.0-t).reshape([t.size()[0], 1])\n",
    "\n",
    "    if initial == False and boundary == False:\n",
    "        term1 = net(X,layer_depth)\n",
    "        model_u_temp = term1\n",
    "    elif initial == False and boundary == True:\n",
    "        # term1 = net(X,layer_depth) * x_temp * x_minus\n",
    "        term1 = torch.mul(net(X,layer_depth),torch.mul(x_temp,x_minus))\n",
    "        term2 = torch.mul(g_0x(X),x_minus)\n",
    "        term3 = torch.mul(g_1x(X),x_temp)\n",
    "        model_u_temp =  torch.add(torch.add(term1,term2),term3)\n",
    "    else:\n",
    "        term1 = torch.mul(net(X,layer_depth),t)\n",
    "        term2 = torch.mul(u_0x(X),t_minus)\n",
    "        model_u_temp =  torch.add(term1,term2)\n",
    "                   \n",
    "    return model_u_temp.reshape([x.size()[0], 1]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmv-WChaVY1F"
   },
   "source": [
    "#### 4.5.1 $ν_1$ generator\n",
    "\n",
    "Generate random points $(x_n, t_n)$ from $[0,1] × [0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KhX--Pm9MpdJ"
   },
   "outputs": [],
   "source": [
    "# generate points by random\n",
    "def generate_sample_one(data_size):\n",
    "    sample_temp = torch.rand(data_size, dimension)\n",
    "    sample_temp.requires_grad = True\n",
    "    return sample_temp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLOpr1KUVvfv"
   },
   "source": [
    "#### 4.5.2 $ν_2$ generator\n",
    "\n",
    "Generate random points $x_n$ from $[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AcOf0J2CMpfs"
   },
   "outputs": [],
   "source": [
    "def generate_sample_two(data_size):\n",
    "    sample_temp = torch.rand(data_size, dimension)\n",
    "    sample_temp[:,1] = 0\n",
    "    sample_temp.requires_grad = True\n",
    "    return sample_temp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 $ν_3$ generator\n",
    "\n",
    "Generate random points $x_n$ from ${\\{0,1\\}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_three(data_size):\n",
    "    sample_temp = torch.rand(data_size, dimension)\n",
    "    sample_temp[0: len(sample_temp)//2, 0] = 0\n",
    "    sample_temp[len(sample_temp)//2: len(sample_temp), 0] = 1\n",
    "    sample_temp.requires_grad = True\n",
    "    return sample_temp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsyCHn1zX8MO"
   },
   "source": [
    "### 4.6 Define Loss function\n",
    "\n",
    "Loss for training on initial and boundary:$$Loss(û) = ||{\\frac{∂û}{∂t}} +  \\hat{u} {\\frac{∂û}{∂x}} - \\nu \\frac{\\partial^{2} û}{\\partial x^{2}}||^2_{[0,1]{×}{[0,1]},{ν_1}} +||û-F(x)||^2_{t=0,[0,1], ν_2} + ||û-g(x,t)||^2_{\\{0,1\\}{×}{[0,1]}, ν_3}$$\n",
    "Loss for training on boundary:$$Loss(û) = ||{\\frac{∂û}{∂t}} +  \\hat{u} {\\frac{∂û}{∂x}} - \\nu \\frac{\\partial^{2} û}{\\partial x^{2}}||^2_{[0,1]{×}{[0,1]},{ν_1}} +||û-F(x)||^2_{t=0,[0,1], ν_2} $$\n",
    "\n",
    "Loss for training on initial:$$Loss(û) = ||{\\frac{∂û}{∂t}} +  \\hat{u} {\\frac{∂û}{∂x}} - \\nu \\frac{\\partial^{2} û}{\\partial x^{2}}||^2_{[0,1]{×}{[0,1]},{ν_1}} + ||û-g(x,t)||^2_{\\{0,1\\}{×}{[0,1]}, ν_3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oV5u1cVsX7xe"
   },
   "outputs": [],
   "source": [
    "\n",
    "# loss function to DGM by auto differential\n",
    "def loss_function(x1, x2, x3, initial=False, boundary=False):\n",
    "    # first term\n",
    "    u_hat = model(x1,initial, boundary)\n",
    "    grad_u_hat = torch.autograd.grad(outputs = u_hat, inputs = x1, grad_outputs = torch.ones(u_hat.shape).to(device), create_graph = True)  # dx and dt\n",
    "    dx = grad_u_hat[0][:, 0]  # dx\n",
    "    dt = grad_u_hat[0][:, 1]  # dt\n",
    "    # dxx and dtt\n",
    "    grad_dx = torch.autograd.grad(outputs = dx, inputs = x1, grad_outputs = torch.ones(dx.shape).to(device).to(device), create_graph = True)\n",
    "    dxx = grad_dx[0][:, 0]\n",
    "    dtt = grad_dx[0][:, 1]\n",
    "    # loss1 = torch.sum((dt + torch.multiply(u_hat.T,dx))**2) / len(x1)\n",
    "    loss1 = torch.add(dt, torch.mul(u_hat.T,dx))\n",
    "    loss1 = torch.sub(loss1, torch.mul(nu, dxx))\n",
    "    loss1 = torch.sum(torch.mul(loss1,loss1)) / len(x1)\n",
    "\n",
    "    # bias = torch.square(torch.mean(dt + torch.multiply(u_hat.T,dx)))\n",
    "    # variance = torch.mean(torch.square(dt + torch.multiply(u_hat.T,dx) - torch.mean(dt + torch.multiply(u_hat.T,dx))))\n",
    "    #second term\n",
    "    if initial == False:\n",
    "        u_hat2 = model(x2,initial, boundary)\n",
    "        u0 = u_0x(x2)\n",
    "        loss2 = torch.sub(u_hat2, u0)\n",
    "        loss2 = torch.sum(torch.mul(loss2,loss2)) / len(x2)\n",
    "    else:\n",
    "        loss2 = 0\n",
    "\n",
    "    #third term\n",
    "\n",
    "    if boundary == False:\n",
    "        u_hat3 = model(x3,initial, boundary)\n",
    "        u_ex3 = g_x(x3)\n",
    "        loss3 = torch.sub(u_hat3, u_ex3)\n",
    "        loss3 = torch.sum(torch.mul(loss3,loss3)) / len(x3)\n",
    "    else:\n",
    "        loss3 = 0\n",
    "    return loss1 + loss2 + loss3\n",
    "    # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1bgW8iaCF0"
   },
   "source": [
    "## 5.Doing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_epoch(loss_record_all, error_record_all,epoch, var_loss, var_error,nu):\n",
    "    para = nu\n",
    "    fig = plt.figure()\n",
    "    # Set subplots size\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(20)\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    # plot a smooth curve of loss\n",
    "    ax.plot(np.arange(epoch), loss_record_all[0])\n",
    "    ax.plot(np.arange(epoch), loss_record_all[1])\n",
    "    ax.plot(np.arange(epoch), loss_record_all[2])\n",
    "    # append error bar to the plot \n",
    "    ax.errorbar(np.arange(epoch), loss_record_all[0], yerr=var_loss[0], label='vanilla')\n",
    "    ax.errorbar(np.arange(epoch), loss_record_all[1], yerr=var_loss[1], label='boundary-included')\n",
    "    ax.errorbar(np.arange(epoch), loss_record_all[2], yerr=var_loss[2], label='initial-included')\n",
    "\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('Empirical loss')\n",
    "    ax.set_title(\"Empirical loss for 1D Burgers' equation, epoch = 30000, nu: {}, lr = 0.0001, data_size = 3000\".format(para), fontsize=12)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = fig.add_subplot(2, 1, 2)\n",
    "    # plot a smooth curve of loss\n",
    "    ax.plot(np.arange(epoch), error_record_all[0])\n",
    "    ax.plot(np.arange(epoch), error_record_all[1])\n",
    "    ax.plot(np.arange(epoch), error_record_all[2])\n",
    "    # append error bar to the plot\n",
    "    ax.errorbar(np.arange(epoch), error_record_all[0], yerr=var_error[0], label='vanilla')\n",
    "    ax.errorbar(np.arange(epoch), error_record_all[1], yerr=var_error[1], label='boundary-included')\n",
    "    ax.errorbar(np.arange(epoch), error_record_all[2], yerr=var_error[2], label='initial-included')\n",
    "\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('Fractional error w.r.t true solution')\n",
    "    ax.set_title(\"Fractional error w.r.t true solution, epoch = 30000, nu: {}, lr = 0.0001,data_size = 3000\".format(para), fontsize=12)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.savefig('loss_epoch,1D, epoch = 30000, nu: {}, width=300, lr = 0.0001,data_size = 3000.png'.format(para))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing data\n",
    "test_data = torch.zeros((101, 101, 2))\n",
    "for i in range(101):\n",
    "  x_test = i / 100\n",
    "  for j in range(101):\n",
    "    t_test = j / 100\n",
    "    test_data[i][j][0] = x_test\n",
    "    test_data[i][j][1] = t_test\n",
    "test_data = torch.reshape(test_data, (101*101, 2)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "def cal_error(x,initial = True, boundary = True):\n",
    "    u_hat = model(x,initial, boundary)\n",
    "    u = u_ex(x)\n",
    "    error = ((u_hat - u).norm(2))**2 / (u.norm(2))**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFxchVN8X70c",
    "outputId": "9b021fa0-f6a9-4b62-bdcc-51366812fd44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "current nu_list: 0.01\n",
      "current run: 0\n",
      "current epoch is:  00\n",
      "current loss is:  tensor(1.9909)\n",
      "current epoch is:  25000\n",
      "current loss is:  tensor(0.0002)\n",
      "current epoch is:  50000\n",
      "current loss is:  tensor(0.0004)\n",
      "Now epoch number is:5616\r"
     ]
    }
   ],
   "source": [
    "epoch = 30000\n",
    "import pickle\n",
    "# layer_width_list = [4,40,200,300,1000,2000,3000]\n",
    "nu_list = [0.01,0.1]\n",
    "print(layer_width)\n",
    "\n",
    "for i in range(len(nu_list)):\n",
    "  nu = nu_list[i]\n",
    "  print(\"current nu_list: {}\".format(nu))\n",
    "  loss_record_all = [[],[],[]]\n",
    "  error_record_all = [[],[],[]]\n",
    "  for i in range(3):\n",
    "    print(\"current run: {}\".format(i))\n",
    "    net = Net(input_width,layer_width, layer_depth).to(device)\n",
    "    initialize_weights(net)\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    # %%\n",
    "    loss_record = np.zeros(epoch)\n",
    "    error_record = np.zeros(epoch)\n",
    "    data_size = 3000\n",
    "    x1 = generate_sample_one(data_size//3)\n",
    "    x2 = generate_sample_two(data_size//3)\n",
    "    x3 = generate_sample_three(data_size//3)\n",
    "    time_start = time.time()\n",
    "    for i in range(epoch):\n",
    "        # scale_y[i] = float(model(scale_x))\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(x1, x2, x3, initial = False, boundary=False)\n",
    "        loss_record[i] = float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        error_record[i] = cal_error(test_data, initial=False, boundary=False)\n",
    "        torch.cuda.empty_cache() # clear memory\n",
    "        print(\"Now epoch number is:{}\".format(i),end='\\r')\n",
    "        if i % 2500 == 0:\n",
    "          # y2.append(model(x2))\n",
    "          print(\"current epoch is: \", i)\n",
    "          print(\"current loss is: \", loss.detach())\n",
    "    time_end = time.time()\n",
    "    print('total time is: ', time_end-time_start, 'seconds')\n",
    "    # plot_heatmap(\"venilla\",initial=False, boundary=False)\n",
    "    loss_record_all[0].append(loss_record)\n",
    "    error_record_all[0].append(error_record)\n",
    "\n",
    "    net = Net(input_width,layer_width, layer_depth).to(device)\n",
    "    initialize_weights(net)\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    # %%\n",
    "    loss_record = np.zeros(epoch)\n",
    "    error_record = np.zeros(epoch)\n",
    "    time_start = time.time()\n",
    "    for i in range(epoch):\n",
    "        # scale_y[i] = float(model(scale_x))\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(x1, x2, x3, initial = False, boundary=True)\n",
    "        loss_record[i] = float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        error_record[i] = cal_error(test_data, initial=False, boundary=True)\n",
    "        torch.cuda.empty_cache() # clear memory\n",
    "        print(\"Now epoch number is:{}\".format(i),end='\\r')\n",
    "        if i % 2500 == 0:\n",
    "          # y2.append(model(x2))\n",
    "          print(\"current epoch is: \", i)\n",
    "          print(\"current loss is: \", loss.detach())\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('total time is: ', time_end-time_start, 'seconds')\n",
    "    loss_record_all[1].append(loss_record)\n",
    "    error_record_all[1].append(error_record)\n",
    "    # plot_heatmap(\"boundary-included\",initial=False, boundary=True)\n",
    "\n",
    "\n",
    "    net = Net(input_width,layer_width, layer_depth).to(device)\n",
    "    initialize_weights(net)\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    # %%\n",
    "    loss_record = np.zeros(epoch)\n",
    "    error_record = np.zeros(epoch)\n",
    "    time_start = time.time()\n",
    "    for i in range(epoch):\n",
    "        # scale_y[i] = float(model(scale_x))\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(x1, x2, x3, initial = True, boundary=False)\n",
    "        loss_record[i] = float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        error_record[i] = cal_error(test_data, initial=True, boundary=False)\n",
    "        torch.cuda.empty_cache() # clear memory\n",
    "        print(\"Now epoch number is:{}\".format(i),end='\\r')\n",
    "        if i % 2500 == 0:\n",
    "          # y2.append(model(x2))\n",
    "          print(\"current epoch is: \", i)\n",
    "          print(\"current loss is: \", loss.detach())\n",
    "    time_end = time.time()\n",
    "    print('total time is: ', time_end-time_start, 'seconds')\n",
    "    # plot_heatmap(\"initial-included\",initial=True, boundary=False)\n",
    "    loss_record_all[2].append(loss_record)\n",
    "    error_record_all[2].append(error_record)\n",
    "\n",
    "  with open('1D_burger_loss_record_all_nu={}width=300.pkl'.format(nu), 'wb') as f:\n",
    "      pickle.dump([loss_record_all,error_record_all], f)\n",
    "\n",
    "  avg_loss_record = []\n",
    "  avg_error_record = []\n",
    "  var_error_record = []\n",
    "  var_loss_record = []\n",
    "  for i in range(len(loss_record_all)):\n",
    "      avg_loss_record.append(np.mean(loss_record_all[i], axis=0))\n",
    "      avg_error_record.append(np.mean(error_record_all[i], axis=0))\n",
    "      var_error_record.append(np.var(error_record_all[i], axis=0))\n",
    "      var_loss_record.append(np.var(loss_record_all[i], axis=0))\n",
    "\n",
    "  plot_loss_epoch(avg_loss_record, avg_error_record, epoch,var_loss_record, var_error_record, nu)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('1D_burger_loss_record_all_nu=1.pkl', 'rb') as f:\n",
    "    a = pickle.load(f)\n",
    "loss_record_all = a[0]\n",
    "error_record_all = a[1]\n",
    "avg_loss_record = []\n",
    "avg_error_record = []\n",
    "\n",
    "var_error_record = []\n",
    "var_loss_record = []\n",
    "for i in range(len(loss_record_all)):\n",
    "    avg_loss_record.append(np.mean(loss_record_all[i], axis=0))\n",
    "    avg_error_record.append(np.mean(error_record_all[i], axis=0))\n",
    "    var_error_record.append(np.var(error_record_all[i], axis=0))\n",
    "    var_loss_record.append(np.var(loss_record_all[i], axis=0))\n",
    "\n",
    "plot_loss_epoch(avg_loss_record, avg_error_record, epoch,var_loss_record, var_error_record,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "0561e18150fb767603442ab0e1c04b7e721a7a0a0aaf96cab6732b70b8e6f592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
